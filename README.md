# MPhil Data Intensive Science Applications of Machine Learning Coursework

This repository contains all the code used for generating figures and results in the report, as well as the neural network models.

## Installation
### Downloading the repository

To download the repository, simply clone it from the GitLab page:

Clone with SSH:
```
git clone git@gitlab.developers.cam.ac.uk:phy/data-intensive-science-mphil/M2_Assessment/dw661.git
```

Clone with HTTPS:
```
git clone https://gitlab.developers.cam.ac.uk/phy/data-intensive-science-mphil/M2_Assessment/dw661.git
```

## Usage
### Pretrained Checkpoints
In the repository, checkpoints for pretrained models are included. All these checkpoints have been trained to the $50$-th epoch. The  checkpoints in the repository include:
- Default DDPM model, trained using default DDPM schedule. It's stored at `./DDPM_checkpoints/CNN_checkpoints/linear_checkpoints/linear_epoch050.pt`.
- Default DDPM model, trained using the Cosine schedule. It's stored at `./DDPM_checkpoints/CNN_checkpoints/cosine_checkpoints/cosine_epoch050.pt`. 
- Default DDPM model, trained using the Cosine schedule and custom UNet model. It's stored at `DDPM_checkpoints/UNet_checkpoints/cosine_checkpoints/cosine_epoch050.pt`.
- Custom ColdDiffusion morphing model, trained using Cosine schedule and custom UNet architecture. It's stored at `ColdDiffusion_checkpoints/UNet_checkpoints/cosine_checkpoints/cosine_epoch050.pt`.
- Custom ColdDiffusion morphing model, trained using origian DDPM schedule and custom UNet architecture. It's stored at `ColdDiffusion_checkpoints/UNet_checkpoints/linear_checkpoints/linear_epoch050.pt`.

### Training the model
To train the model, run `python src/train_model.py` with three command line arguments. 
- The first argument is the choice of schedule, valid choices include: `linear` (which is the default noise schedule in starter notebook), `cosine`, `constant` and `inverse`.
- The second argument is the type of diffusion model to use, valid choices include: `DDPM` (which is the default diffusion model in the notebook) and `ColdDiffusion`.
- The third argument is the type of neural network architecture to use. Valid choices include: `CNN` (which is the default network structure in the starter notebook) and `UNet`.

After the model starts training, generated images, checkpoints and losses will be recorded and automatically saved to corresponding folders. The folder names are in the convention `{model_type}/{nn_architecture}/{choice_of_scheduler}`.

Below are some examples for training the model:
```{Python}
# This will train a DDPM model, using cosine schedule and the default CNN structure
python src/train_network.py cosine DDPM CNN

# This will train a ColdDiffusion model, using linear (DDPM) schedule and the custom UNet structure
python src/train_network.py linear ColdDiffusion UNet
```
### Evaluating the models
After training the models, their FID scores can be calculated. The script for calculating FID scores is included in the file `src/metric_calculation_scripts/calculate_fid_score.py`. Similar to training the network, three command line arguments should be passed to specify which model you would want to evaluate FID score on. The calculated FID scores are outputted to the directory in the form of a csv file: `{model_type}_results/{nn_architecture}_results/fid_scores_{choice_of_scheduler}.csv`.

Below is an example for evaluating a model:
```{Python}
# This will evaluate the fid scores of the DDPM model with cosine schedule and CNN
python src/metirc_calculation_scripts/calculate_fid_score.py cosine DDPM CNN
```

### Plot Generating scripts
In the repository, all scripts used for generating plots are included under the directories `src/plotting_scripts` and `src/sampling_scripts`. The docstrings in the scripts indicate which figures in the report were generated by these plots. All plots are saved to a folder called `figures`.

## Docker Instructions
All the packages used and their versions were included in the file `environments.yml`. This file can be used to replicate the Conda environment used during the development of this repository.

To run the Python scripts inside Docker, first build the image

```
Docker build -t m2 .
```
This would generate an image called `m2`. To deploy and run the container, run the following command:

```
Docker run --rm -ti m2
```
This would start the process inside the container.

## Hardware Specifications
Training the custom ColdDiffusion model with UNet as the neural network takes around $30$ minutes on a 3080 GPU; the starter notebook takes around $10$ minutes. The network training script isn't dependent on cuda; however, the fid score calculation scripts are exclusively for cuda, as it's extremely slow to run on CPU's and the model isn't compatible with Apple's chip.

## Contributing
Pull requests are welcome. For major changes, please open an issue first
to discuss what you would like to change.

Please make sure to update tests as appropriate.

## License

[MIT](https://choosealicense.com/licenses/mit/)

## Acknowledgement of Generative AI Tools
During the completion of coursework, generative tools such as ChatGPT and CoPilot were used supportively and minimally. All code involving any algorithms or calculations were entirely produced by myself; Copilot was only partially used for Docstring and plotting, and ChatGPT was only used for latex syntax queries. Examples of prompts include:

"How to create a $3 \times 3$ subplot in Latex?"

"How can I get the first two images from the MNIST dataset?"